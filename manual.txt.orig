## About authors
Adarsh Barik: abarik@purdue.edu
Mohith Murlidhar: mmurlidh@purdue.edu

## About data set
(Requires Kaggle login)
Training images:  https://www.kaggle.com/c/street-view-getting-started-with-julia/download/trainResized.zip
Training labels: https://www.kaggle.com/c/street-view-getting-started-with-julia/download/trainLabels.csv
Test images: https://www.kaggle.com/c/street-view-getting-started-with-julia/download/testResized.zip

## About the code
1. Operating System
Code is written and tested in Linux oeprating system. More precisely, Ubuntu 16.04 with kernel 4.4.0-47-generic. The code is NOT tested in windows/Mac.
2. Programing Language 
Code is written using Python 2.7.12, we have tried to make it compatible with python 3.0+ as well but all our testing has been done on computer running python 2.7.12. This is absolutely important as many of the libraries/modules used in the code depend on python version (also read How to Improve Running Time).
3. Required Libraries
sklearn : version 0.18
skimage : 0.12.3 
cPickle : (pickle if using python 3+)
ConfigParser :  (configparser if using python 3+)
sys
os
csv
numpy
pandas
itertools
random
matplotlib

To install sklearn use: $ pip install --user -U scikit-learn
To install skimage use: $ pip install --user -U scikit-image
Other packages can also be installed using $ pip install --user -U <package_name>
Please make sure that you have correct version of sklearn and skimage installed. Other packages work at their current stable verison. 

## Directory structure
.
├── configuration
├── data
│   ├── storage
│   │   ├── figures
│   │   ├── ovr
│   │   │   └── figures
│   │   ├── samplesize
│   │   │   ├── _100
│   │   │   │   └── figures
│   │   │   ├── _1000
│   │   │   │   └── figures
│   │   │   ├── _2000
│   │   │   │   └── figures
│   │   │   ├── _3000
│   │   │   │   └── figures
│   │   │   ├── _4000
│   │   │   │   └── figures
│   │   │   ├── _50
│   │   │   │   └── figures
│   │   │   ├── _500
│   │   │   │   └── figures
│   │   │   ├── _5000
│   │   │   │   └── figures
│   │   │   └── _6000
│   │   │       └── figures
│   ├── test
│   │   └── keys
│   ├── testResized
│   ├── testResizedKeys
│   ├── trainResized
│   └── trainResizedKeys
├── report
├── source

<<<<<<< HEAD
configuration - folder containing the config file for configuring the initial settings and parameters
data - folder containing all the data,both existing and generated.
storage - folder in data containing the data generated from the program i.e. plots, csvs etc.
figures - folder in storage containing the plots and csv's of bootstrap and kfold cross validation (one vs one)
ovr - the folder in storage contains the one vs rest analysis plots and csvs for kfold cross validation
samplesize ,_100,_500 etc. - folder in storage containing the analysis of varying sample sizes across kernels, plots and csvs
test - folder in data containing the 100 resized keys used for testing for unlabelled data
testResized - folder in data containing all the test resized images
testResizedKeys - folder in data containing all the test resized keys
trainResized - folder in data containing all the train resized data images
trainResizedKeys - folder in data containing all the train resized keys
report - final report for project in pdf format
source - folder containing all the code necessary to perform image processing, generate bag of words, run the model and perform cross validation

## How to Run the Code

## How to Improve Running Time
||||||| merged common ancestors
=======
## How to Run the Code
1. All experiements are in the file main.py inside "source" directory which can be run by going to source directory and running the command:
$ python main.py
2. Now we'll explain what deos this python script do in more detail. As first step, it calls function preprocess()
3. preprocess() function is defined in "preprocessing.py" file. It does the following job:
	i. Converts training images to corresponding vectors
	ii. Generates bag of words for future reference
	iii. Returns vectors of images, their labels and corresponding image names
We STRONGLY recommend to run preprocess() atleast once while running in any new setup. After its first successful run, we can go to "configuration" directory and  edit the "config" file in following way:
DO_PREPROCESSING = False
After this change, in the next run preprocess() will load already generated and stored training image vectors, training image labels and corresponding training image names.
The above is also important as we store some objects as pickle files (<file>.p extension) and these files depend heavily on the version of python, hence they need to be genrated during first run of preprocess() in any new setup.
4. preprocess() uses "sift" binary inside "source" directory to generate SIFT keys and descriptors and hence it must have execution permission BEFORE the first run of the code. This can be done by going to "source" directory and running the command:
$ chmod  a+x sift
5. We define parameters for various kernels in the next few line of code. They have been named "parameters_<kernel_type>" and are "dictionaries" with values of type "list". They can be changed as per the requirement of the experiment. 
6. If we wish to skip a particular kernel then they can be just defined as:
parameters_<kernel_type> = None
7. Next we start the cross validation process by calling function start_cross_validation(image_vectors, image_labels, out_dir, cv_type='kfold', cv_n=3, param_lin=None, param_poly=None, param_rbf=None, svm_type=None)
8. start_cross_validation(..) is defined in "cross_validation_cl.py" file in "source" directory.
9. start_cross_validation(..) can run cross validation for various combinations of kernels and parameters and svm type.
10. On its first call, we run it for cv_type='kfold' (we can skip this argument as by default it runs for kfolds)
11. On its second call, we run it for cv_type='bootstrap'
12. On its third call, we run it for svm type one-vs-rest by giving argument svm_type='ovr'
13. We can specify number of folds/B by giving argument cv_n=<number of folds/ B> , by default it takes value as 3
14.  It stores the results as csv files in out_dir and generated all possible plots for hyperparameter estimation
15. At last we call the function effect_of_sample_size(image_vectors, image_labels, out_dir, n_range=[50, 1000, 3000, 6000], param_lin=None, param_poly=None, param_rbf=None) which is defined in the file "effect_of_sample_size.py" in "source" directory.
16. This function runs the start_cross_validation(..) routine for truncated size of training vectors which can be controlled by argument n_range
17.  Once we have selected our model, we can test our for unlabelled samples by running following command:
$ python testing_unlabelled_data.py
18. This reads unlabelled test image names from a csv files provided in "config" file within "configuration" folder. Check for:
TEST_CSV = ../data/test/testLabels.csv
19. It also makes use of stored pickle file to generate bag of words, hence it is important that file "kmeans_model.p" exists (and of correct version) inside directory "data/storage"
20. It prints the accuracy it achieved in standard output and stores the results in a csv file named "unlabelled_results.csv'" inside directory "data/storage"

## How to Improve Running Time
1. Change DO_PREPROCESSING to False in "config" files inside "configuration" directory AFTER the first successfull run (read above section to get a detailed explaination for this)
2. Choose parameters_<kernel_type> based on computational resources available (as a general rule, the less number of parameters to be tested, less the running time)
3. Comment out the experiements which you don't want to run. For example, we don't need to run start_cross_validation for both kfolds and bootstrap. We can also skip change of svm type
4. Run effect_of_sample_size(..) only when required. It goes through many loops (loops for sample size, then kernels, then hyperparameters). If not needed then comment it out.

>>>>>>> fd90b74420b0c923c07277ebdf32b584ccbda2ad
